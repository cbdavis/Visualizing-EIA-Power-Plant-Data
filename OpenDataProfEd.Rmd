---
title: "Data Visualization and Analysis Tutorial"
author: "Chris Davis"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The shortened URL for this page is **[http://is.gd/ODGU2016](http://is.gd/ODGU2016)**

The journal article **[The State of the States: Data-driven Analysis of the US Clean Power Plan](http://www.sciencedirect.com/science/article/pii/S1364032116001271)** shows applications of the analysis and visualization shown here


### Introduction

There are a variety of different tools available which are useful for data visualization and analysis.  For dealing with large amounts of data you may see people using [R](https://www.r-project.org/) or [Python](https://www.python.org/).  For visualization on the web, you often see JavaScript libraries just as [d3.js](https://d3js.org/).

What we'll focus on today is an example using the R programming language and two of its libraries which are commonly used for data analysis and visualization: dplyr and ggplot2.  R is a programming language that is commonly used for data analysis.  As R is open source, people are also free to contribute libraries to it that help to extend its functionality in ways that allow you to do new things, or make it easier to do existing things.  Currently there are nearly [9000 packages](https://cran.r-project.org/web/packages/) meaning that you can often reuse code instead of having to create something yourself.

For both of the dplyr and ggplot2 libraries, there are reference sheets available which give a quick overview of what they're capable of.  Today we'll be showing only a small subset of their functionality.

* [Data Visualization with ggplot2 Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)
* [Data Wrangling with dplyr and tidyr Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

---

### Learning Objectives
In this tutorial we will show how to use these open source libraries to be able to explore large complex datasets.  More specifically, after this lecture you should be familiar with several methods of filtering, grouping and summarizing data, in addition to different ways of visualizing the data.  Given a complex dataset of your own, you should be able to describe different options that would allow you to explore the data.

---

### Examples
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(ggplot2)
library(dplyr)
library(lubridate)
library(knitr)
```

To start off, we'll look at a data set that is included with R.  This is a [famous data set collected in 1936 by the statistician Ronald Fisher](https://en.wikipedia.org/wiki/Iris_flower_data_set) that documents measurements of different species of iris flowers.  It's an interesting data set since the different species have similar measurements, and in some cases it can be difficult to tell them apart simply from their measurements.

Tabular data like you would find in spreadsheets is represented within R in what are known as data frames.  

Looking at the first few lines of the `iris` data frame, we see the following.  There are five columns: `Sepal.Length`, `Sepal.Width`, `Petal.Length`, `Petal.Width` and `Species`
```{r, eval=FALSE}
head(iris)
```

```{r echo=FALSE}
kable(head(iris))
```

One of the nice things about data frames is that you can use the names of the columns (combined with the `$` sign) to directly access the values in that column:

```{r}
iris$Sepal.Length
```

---

### ggplot2
Many visualizations created with R are often created using the [ggplot2 library](http://ggplot2.org/).  What's interesting about this library is that way in which it allows you to construct visualizations.  The gg in ggplot2 stands for the Grammar of Graphics. The idea is that when you create plots, you are basically writing sentences that are of the form:

`Here's my data frame` + `Here are the x and y columns` + `Apply this kind of plot to that data` + `These are the axis labels` + `here are some more additional transformations`

The syntax may look strange at first, although itâ€™s a very modular approach, and you can create very complex visualizations just by adding new parts to these sentences.

---

To visualize this, we can first do a simple scatter plot.  You'll notice with the syntax that we first start with the `iris` data frame, then we specify which columns are to be associated with the `x` and `y` values, and then we specify that we want to plot the data as points by adding `+ geom_point()`.

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point()
```

In the following examples, you may see the code examples split over multiple lines.  The two statements below are actually equivalent, but by spreading the commands over multiple lines it can sometimes help to make things more readable by separating the code into its different functional pieces.
```{r eval=FALSE}
# same code in 1 line
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + geom_point()

# same code in 4 lines
ggplot(iris, 
       aes(x=Sepal.Length, 
           y=Sepal.Width)) + 
  geom_point()
```

---

In addition to specifying which columns represent `x` and `y` values, we can also specify which one will be used for the `colour` of the points:

```{r irisPointColor, cache=TRUE}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, colour=Species)) + 
  geom_point()
```

---

And we can also add an additional statement, `facet_wrap`, to indicate that we want separate plots for each of the data per species.

```{r irisFacetWrap, cache=TRUE, fig.width=8, fig.height=4}
ggplot(iris, aes(x=Sepal.Length, 
                 y=Sepal.Width, 
                 colour=Species)) + 
  geom_point() + 
  facet_wrap(~Species)
```

---

Instead of just having numeric values on the axes, we can also use categorical values such as the species, and then use this to create a box plot:

```{r irisBoxplot, cache=TRUE}
ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_boxplot()
```

---

or change this to a violin plot, which is a similar concept, but shows the number of points at a particular value by the width of the shapes.
```{r irisViolin, cache=TRUE}
ggplot(iris, aes(x=Species, y=Sepal.Length)) + geom_violin()
```

---

For analyzing the data, we can use the dplyr library.  The dplyr library takes a very similar approach to what we see with ggplot.  We start off with data represented in a table, or in what R calls a data frame, and then we perform a series of operations or data transformations which are connected together via the `%>%` symbol which acts as a sort of "pipe" through which data flows.  

In very general terms, we may create a series of statements that look like:

data `%>%` operation 1 `%>%` operation 2

---

As an example of what this can look like, using the iris data set again, we do the following: 

1. group the data by species using `group_by(Species)`
1. per species we calculate the mean, standard deviation, minimum, and maximum sepal lengths using the `summarize` command.

This creates new columns such as `mean_sepal_length` which is calculated via the function `mean(Sepal.Length)`

```{r eval=FALSE}
iris %>% 
  group_by(Species) %>% 
  summarize(mean_sepal_length = mean(Sepal.Length), 
            sd_sepal_length = sd(Sepal.Length), 
            min_sepal_length = min(Sepal.Length),
            max_sepal_length = max(Sepal.Length))
```

```{r echo=FALSE}
kable(iris %>% 
  group_by(Species) %>% 
  summarize(mean_sepal_length = mean(Sepal.Length), 
            sd_sepal_length = sd(Sepal.Length), 
            min_sepal_length = min(Sepal.Length),
            max_sepal_length = max(Sepal.Length)))
```

```{r loadPowerPlantData, echo=FALSE, cache=TRUE}
load("./data/generationAndFuelData.RData")
generationAndFuelData = generationAndFuelData %>% filter(!is.na(state), !is.na(netgen))
```

---

### Advanced Example

For a more complicated example, we use a data set on electricity generation in the US.  This data contains around 1.5 million observations on monthly generation from individual generators for the years 2001 through 2014.  Here we see a preview of the data where we only look at the columns for `plant_name`, `state`, `general_fuel_name` (the fuel type), `date`, and `netgen` (the amount of electricity generated).


```{r, eval=FALSE}
head(generationAndFuelData[,c("plant_name", "state", "general_fuel_name", "date", "netgen")])
```

```{r showDataSample, cache=TRUE, echo=FALSE}
kable(head(generationAndFuelData[,c("plant_name", "state", "general_fuel_name", "date", "netgen")]))
```

---

This is too much data to visualize, so we'll have to summarize it first.  As an example, we 

1. filter by all the generators in the `state` of Texas
1. group all values by `date` and `general_fuel_name`
1. sum up the total amount of electricity generation by date (per month in this case) and by the `general_fuel_name`.

```{r texasGenerationFilter, cache=TRUE}
tmp = generationAndFuelData %>% 
  filter(state == "TX") %>% 
  group_by(date, general_fuel_name) %>% 
  summarize(total_generation = sum(netgen, na.rm=TRUE))
```

Next we plot the values where the x axis is the `date`, the y axis shows the `total_generation` per date and fuel as calculated previously, and we color the data by the `general_fuel_name`.  We then use `+ geom_line()` to specify that lines should be drawn for the data.  Because we specified `colour=general_fuel_name` this means that the lines will be grouped by the `general_fuel_name`.
```{r texasGenerationLines, cache=TRUE}
ggplot(tmp, aes(x=date, 
                y=total_generation, 
                colour=general_fuel_name)) + 
  geom_line()

```

---

We can also stack the values per date so we can get an overview of how much generation from each fuel type contributes to the whole.
```{r}
ggplot(tmp, aes(x=date, 
                y=total_generation, 
                fill=general_fuel_name)) + 
  geom_area()
```

---

What we see with this is that there's a lot of variation over the course of a year, likely due to aspects such as increased air conditioning demand in the summer.  To smooth this out, we can also sum up the total amount of generation per fuel type per year.

In the steps below we:

1. Keep all data for generators in Texas (`state == "TX"`)
1. Use `mutate` to add a new column called `year` that is derived from the year value of the date
1. Group all data into blocks by the year and fuel type
1. Per block, sum up the total amount of generation.

```{r texasYearlyGenStackedGraph, cache=TRUE}
tmp = generationAndFuelData %>% 
  filter(state == "TX") %>% 
  mutate(year = year(date)) %>%
  group_by(year, general_fuel_name) %>% 
  summarize(total_generation = sum(netgen))

ggplot(tmp, aes(x=year, 
                y=total_generation, 
                fill=general_fuel_name)) + 
  geom_area()
```

---

We can also do this for the entire US by using the `facet_wrap` command to create a plot for every single `state`.  Note how we specify `scales="free_y"` for the facet wrap so that the y axes for the plots are scaled to the maximum values in each states.  Without this, we would only be able to clearly see the data for the states that are the largest producers of electricity.

From this plot, we can see the diversity across the US states in terms of the types of fuel used.  It also becomes clear which states have changed their fuel types over time.  In particular, you can see large amounts of wind capacity being installed in the states in the Midwest of the country, and a general shift away from coal to increased use of natural gas.

```{r allStatesFacetWrap, fig.width=16, fig.height=10, cache=TRUE}
tmp = generationAndFuelData %>% 
  mutate(year = year(date)) %>%
  group_by(year, general_fuel_name, state) %>% 
  summarize(total_generation = sum(netgen))

ggplot(tmp, aes(x=year, 
                y=total_generation, 
                fill=general_fuel_name)) + 
  geom_area() + 
  facet_wrap(~state, scales="free_y")
```

---

Similarly, we can analyze yearly generation from different sources for the entire US.  Again we use a `facet_wrap` with independent scaling of the y axes.  Additionally, `expand_limits` is specified to make sure that the minimum value shown on the y axes is zero.

What we can see with these graphs is that generation from coal is going down, while that from natural gas is increasing.  It's also interesting to note that generation from solar and wind are increasing at a very fast pace, although they are still only a small part of total 

```{r entireUSGenerationByFuelType, cache=TRUE, fig.width=10, fig.height=4}
tmp = generationAndFuelData %>% 
  mutate(year = year(date)) %>%
  group_by(year, general_fuel_name) %>% 
  summarize(total_generation = sum(netgen))

ggplot(tmp, aes(x=year, y=total_generation)) + 
  geom_line() + 
  facet_wrap(~general_fuel_name, scales="free_y", ncol=5) + 
  expand_limits(y=0)
```

---

We can also find out which states were the top generators of electricity from wind in 2014

1. `mutate` lets us add new columns based on values in other columns.  The `date` column contains values such as `12-15-2014`.  By using `year = year(date)` we create a new column `year` that is based on the year value of the date, or in the example of `12-15-2014`, `2014`
1. We `filter` for all generation data where the `general_fuel_name` is `Wind` and for where the `year` is `2014`.
1. We use `group_by` to group all the data into blocks per `state`.
1. Per block, we `summarize` to create a new column `total_generation` which is the sum of the values in the `netgen` column
1. Finally we sort the results based on the descending values of `total_generation`

```{r topWindStates2014}
tmp = generationAndFuelData %>% 
  mutate(year = year(date)) %>%
  filter(general_fuel_name == "Wind", year == 2014) %>%
  group_by(state) %>%
  summarize(total_generation = sum(netgen)) %>% 
  arrange(desc(total_generation))

```

```{r eval=FALSE}
head(as.data.frame(tmp))
```

```{r echo=FALSE}
kable(head(as.data.frame(tmp)))
```

---

### Conclusion

This has been just a brief introduction into data analysis and visualization, and there is clearly much more that is possible.  When choosing the types of tools that you will use for this, it is also important to think not just in terms of programming languages, but also about the communities around them.  

With languages such as R, Python and JavaScript, there are large numbers of people who are actively using these, which means that it is easy to find documentation and help.  Furthermore, the open source nature of these means that people are continually developing new libraries which help to provide additional functionality that may be useful for you.  In practice, you may find yourself using several libraries developed by different people, which help you to manage aspects of data gathering, analysis and visualization.